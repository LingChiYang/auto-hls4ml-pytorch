{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "\n",
    "class NDT4HLS(torch.nn.Module):\n",
    "    def __init__(self, config, trial_length, num_neurons, device, max_spikes):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.trial_length = trial_length\n",
    "        self.num_neurons = num_neurons\n",
    "        self.device = device\n",
    "\n",
    "        # TODO buffer\n",
    "        if config.FULL_CONTEXT:\n",
    "            self.src_mask = None\n",
    "        else:\n",
    "            self.src_mask = {} # multi-GPU masks\n",
    "        if config.EMBED_DIM == 0:\n",
    "            self.num_input = num_neurons\n",
    "        else:\n",
    "            self.num_input = config.EMBED_DIM * num_neurons\n",
    "\n",
    "        self._init_transformer()\n",
    "        self.src_mask = self._get_or_generate_context_mask(torch.zeros(self.trial_length, self.trial_length, device=device))\n",
    "        self.padding_mask = torch.randint(0, 2, (self.trial_length, self.trial_length), device=device).bool()\n",
    "        #self.init_weights()\n",
    "\n",
    "    def _init_transformer(self):\n",
    "        norm = nn.LayerNorm(self.num_input)\n",
    "        #encoder_layer = nn.TransformerEncoderLayer(d_model=self.num_input, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=self.num_input, \n",
    "                                       nhead=self.config.NUM_HEADS,\n",
    "                                       dim_feedforward=self.config.HIDDEN_SIZE,\n",
    "                                       dropout=self.config.DROPOUT,\n",
    "                                       activation=self.config.ACTIVATION,\n",
    "                                       norm_first=self.config.PRE_NORM,\n",
    "                                       device=self.device),\n",
    "            self.config.NUM_LAYERS,\n",
    "            norm=norm\n",
    "        )\n",
    "    \n",
    "    def _get_or_generate_context_mask(self, src, do_convert=True, expose_ic=True):\n",
    "        if self.config.FULL_CONTEXT:\n",
    "            return None\n",
    "        if str(src.device) in self.src_mask:\n",
    "            return self.src_mask[str(src.device)]\n",
    "        size = src.size(0) # T\n",
    "        context_forward = self.config.CONTEXT_FORWARD\n",
    "        if self.config.CONTEXT_FORWARD < 0:\n",
    "            context_forward = size\n",
    "        mask = (torch.triu(torch.ones(size, size, device=src.device), diagonal=-context_forward) == 1).transpose(0, 1)\n",
    "        if self.config.CONTEXT_BACKWARD > 0:\n",
    "            back_mask = (torch.triu(torch.ones(size, size, device=src.device), diagonal=-self.config.CONTEXT_BACKWARD) == 1)\n",
    "            mask = mask & back_mask\n",
    "        if expose_ic and self.config.CONTEXT_WRAP_INITIAL and self.config.CONTEXT_BACKWARD > 0:\n",
    "            # Expose initial segment for IC\n",
    "            initial_mask = torch.triu(torch.ones(self.config.CONTEXT_BACKWARD, self.config.CONTEXT_BACKWARD, device=src.device))\n",
    "            mask[:self.config.CONTEXT_BACKWARD, :self.config.CONTEXT_BACKWARD] |= initial_mask\n",
    "        mask = mask.float()\n",
    "        def binary_mask_to_attn_mask(x):\n",
    "            return x.float().masked_fill(x == 0, float('-inf')).masked_fill(x == 1, float(0.0))\n",
    "        if do_convert:\n",
    "            mask = binary_mask_to_attn_mask(mask)\n",
    "        self.src_mask[str(src.device)] = mask\n",
    "        return self.src_mask[str(src.device)]\n",
    "\n",
    "    def forward(self, src):  \n",
    "        #src_mask = self._get_or_generate_context_mask(src)\n",
    "        output = self.transformer_encoder(src, mask=self.src_mask)\n",
    "        return output\n",
    "    \n",
    "model = torch.load('./model/NDT4HLS.pth', map_location=torch.device('cpu'))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "from pprint import pprint\n",
    "config = hls4ml.utils.config_from_pytorch_model(model, \n",
    "                                                granularity='name',\n",
    "                                                backend='Vitis',\n",
    "                                                input_shapes=[[1, 180, 182]], \n",
    "                                                default_precision='ap_fixed<18,8,AP_RND_CONV>', \n",
    "                                                inputs_channel_last=True, \n",
    "                                                transpose_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import math\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import numpy as np\n",
    "torch.set_printoptions(precision=15)\n",
    "class TorchQuantizer(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 bitwidth=18, \n",
    "                 int_bitwidth=8, \n",
    "                 signed=True,\n",
    "                 rounding='CONVERGENT',\n",
    "                 saturation='WRAP',\n",
    "                 calibration=False,\n",
    "                 quantize=True,\n",
    "                 dtype=torch.float64):\n",
    "        super(TorchQuantizer, self).__init__()\n",
    "        self.bitwidth = bitwidth\n",
    "        self.int_bitwidth = int_bitwidth\n",
    "        self.signed = signed\n",
    "        self.m = pow(2, self.bitwidth) if not calibration else 1 #in calibration mode, no need to calculate m\n",
    "        self.m_i = pow(2, self.int_bitwidth) if not calibration else 1\n",
    "        self.q = self.m / self.m_i\n",
    "        self.q = float(self.q)\n",
    "        self.lower_bound = -self.m/2 if self.signed else 0\n",
    "        self.upper_bound = self.m/2-1 if self.signed else self.m-1\n",
    "        self.rounding = rounding\n",
    "        self.saturation = saturation\n",
    "        self.calibration = calibration\n",
    "        self.quantize = quantize\n",
    "        self.max_int_bits = torch.tensor(-torch.inf)\n",
    "        self.max_value = torch.tensor(-torch.inf)\n",
    "        self.min_frac_bits = torch.tensor(torch.inf)\n",
    "    def forward(self, x):\n",
    "        if self.quantize == False:\n",
    "            return x\n",
    "        if self.calibration:\n",
    "            x_flat = x.flatten()\n",
    "            x_flat = x_flat[x_flat != 0]\n",
    "            #check if x_flat is not empty\n",
    "            if x_flat.nelement() > 0:\n",
    "                max_int_bits = torch.max(torch.ceil(torch.log2(torch.abs(x_flat))).max())\n",
    "                max_int_bits += 1 if self.signed else 0\n",
    "                min_frac_bits = torch.min(torch.ceil(torch.log2(torch.abs(x_flat))).min())\n",
    "                min_frac_bits += 1 if self.signed else 0\n",
    "                self.max_int_bits = torch.max(max_int_bits, self.max_int_bits).int()\n",
    "                self.min_frac_bits = torch.min(min_frac_bits, self.min_frac_bits).int()\n",
    "            return x\n",
    "        if self.rounding == 'CONVERGENT':\n",
    "            if self.saturation == 'WRAP':\n",
    "                qx = ((torch.round(x * self.q) - self.lower_bound) % (self.upper_bound - self.lower_bound + 1) + self.lower_bound) / self.q\n",
    "            else:\n",
    "                qx = torch.clamp(torch.round(x * self.q), self.lower_bound, self.upper_bound)/self.q\n",
    "        else:\n",
    "            if self.saturation == 'WRAP':\n",
    "                qx = ((torch.trunc(x * self.q) - self.lower_bound) % (self.upper_bound - self.lower_bound + 1) + self.lower_bound) / self.q\n",
    "            else:\n",
    "                qx = torch.clamp(torch.trunc(x * self.q), self.lower_bound, self.upper_bound)/self.q\n",
    "        # if qx == nan, raise expcetion\n",
    "        if torch.isnan(qx).any():\n",
    "            print(\"x:\",x)\n",
    "            raise Exception(\"Quantized value is NaN\")\n",
    "        return qx\n",
    "    def forward_inplace(self, x):\n",
    "        if self.quantize == False:\n",
    "            return x\n",
    "        if self.calibration:\n",
    "            x_flat = x.flatten()\n",
    "            x_flat = x_flat[x_flat != 0]\n",
    "            if x_flat.nelement() > 0:\n",
    "                max_int_bits = torch.max(torch.ceil(torch.log2(torch.abs(x_flat))).max())\n",
    "                max_int_bits += 1 if self.signed else 0\n",
    "                min_frac_bits = torch.min(torch.ceil(torch.log2(torch.abs(x_flat))).min())\n",
    "                min_frac_bits += 1 if self.signed else 0\n",
    "                self.max_int_bits = torch.max(max_int_bits, self.max_int_bits).int()\n",
    "                self.min_frac_bits = torch.min(min_frac_bits, self.min_frac_bits).int()\n",
    "                #self.max_int_bits += 1 if self.signed else 0\n",
    "                #self.min_frac_bits += 1 if self.signed else 0\n",
    "            return x\n",
    "        if self.rounding == 'CONVERGENT':\n",
    "            if self.saturation == 'WRAP':\n",
    "                x.mul_(self.q).round_().sub_(self.lower_bound).remainder_(self.upper_bound - self.lower_bound + 1).add_(self.lower_bound).div_(self.q)\n",
    "            else:\n",
    "                x.mul_(self.q).round_().clamp_(self.lower_bound, self.upper_bound).div_(self.q)\n",
    "        else:\n",
    "            if self.saturation == 'WRAP':\n",
    "                x.mul_(self.q).trunc_().sub_(self.lower_bound).remainder_(self.upper_bound - self.lower_bound + 1).add_(self.lower_bound).div_(self.q)\n",
    "            else:\n",
    "                x.mul_(self.q).trunc_().clamp_(self.lower_bound, self.upper_bound).div_(self.q)\n",
    "        #x.mul_(self.q).round_()\n",
    "        #x.clamp_(self.lower_bound, self.upper_bound)\n",
    "        #x.round_()\n",
    "        #x.div_(self.q)\n",
    "        return x\n",
    "\n",
    "class QLinear(torch.nn.Linear):\n",
    "    def __init__(self, \n",
    "                 in_features:int, \n",
    "                 out_features:int, \n",
    "                 bias:bool=True, \n",
    "                 device=None,\n",
    "                 dtype=torch.float64,\n",
    "                 quant_config:dict=None,\n",
    "                 calibration=False):\n",
    "        super(QLinear, self).__init__(in_features, out_features, bias, device=device, dtype=dtype)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.quant_config = quant_config\n",
    "        self.calibration = calibration\n",
    "        self.weight_qtzr = TorchQuantizer(**quant_config['weight'], calibration=calibration)\n",
    "        self.bias_qtzr = TorchQuantizer(**quant_config['bias'], calibration=calibration)\n",
    "        self.input_qtzr = TorchQuantizer(**quant_config['input'], calibration=calibration)\n",
    "        self.output_qtzr = TorchQuantizer(**quant_config['output'], calibration=calibration)\n",
    "        self.dtpye = dtype\n",
    "        #self.reset_parameters()\n",
    "        \n",
    "    #def reset_parameters(self):\n",
    "    #    #reset to zero\n",
    "    #    torch.nn.init.zeros_(self.weight, dtype=self.dtype)\n",
    "    #    if self.bias is not None:\n",
    "    #        torch.nn.init.zeros_(self.bias, dtype=self.dtype)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        qw = self.weight_qtzr(self.weight)\n",
    "        qx = self.input_qtzr(x)\n",
    "        qy = torch.matmul(qx, qw.t())\n",
    "        if self.bias is not None:\n",
    "            qy += self.bias_qtzr(self.bias)\n",
    "        qy = self.output_qtzr(qy)\n",
    "        return qy\n",
    "    \n",
    "class QFlashMultiheadAttention(torch.nn.MultiheadAttention):\n",
    "    def __init__(self, \n",
    "                 embed_dim:int, \n",
    "                 num_heads:int, \n",
    "                 bias:bool=True, \n",
    "                 batch_first:bool=False, \n",
    "                 device=None, \n",
    "                 dtype=torch.float64,\n",
    "                 quant_config:dict=None,\n",
    "                 token_tile_size:int=1,\n",
    "                 embed_tile_size:int=1,\n",
    "                 head_tile_size:int=1,\n",
    "                 max_neg_value:float=-8.0,\n",
    "                 calibration=False):\n",
    "        super(QFlashMultiheadAttention, self).__init__(embed_dim, \n",
    "                                                  num_heads,  \n",
    "                                                  bias=bias, \n",
    "                                                  add_bias_kv=False, \n",
    "                                                  add_zero_attn=False,\n",
    "                                                  kdim=None, \n",
    "                                                  vdim=None, \n",
    "                                                  batch_first=batch_first, \n",
    "                                                  device=device, \n",
    "                                                  dtype=dtype)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.batch_first = batch_first\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = torch.tensor(1.0 / math.sqrt(self.head_dim))\n",
    "        self.in_proj = QLinear(embed_dim, \n",
    "                               3*embed_dim, \n",
    "                               bias=bias, \n",
    "                               device=device, \n",
    "                               dtype=dtype,\n",
    "                               quant_config=quant_config['in_proj'], calibration=calibration)\n",
    "        self.scale_qtzr = TorchQuantizer(**quant_config['scale'], calibration=calibration)\n",
    "        self.token_tile_size = token_tile_size\n",
    "        self.embed_tile_size = embed_tile_size\n",
    "        self.head_tile_size = head_tile_size\n",
    "        self.max_neg_value = max_neg_value\n",
    "        self.row_sum_qtzr = TorchQuantizer(**quant_config['row_sum'], calibration=calibration)\n",
    "        self.exp_input_qtzr = TorchQuantizer(**quant_config['exp_input'], rounding='TRUNCATE', saturation='SAT', calibration=calibration)\n",
    "        self.exp_output_qtzr = TorchQuantizer(**quant_config['exp_output'], saturation='SAT', calibration=calibration)\n",
    "        self.inv_input_qtzr = TorchQuantizer(**quant_config['inv_input'], rounding='TRUNCATE', saturation='SAT', calibration=calibration)\n",
    "        self.inv_output_qtzr = TorchQuantizer(**quant_config['inv_output'], saturation='SAT', calibration=calibration)\n",
    "        self.attn_out_qtzr = TorchQuantizer(**quant_config['out_proj']['input'], calibration=calibration)\n",
    "        self.out_proj = QLinear(embed_dim, \n",
    "                                embed_dim, \n",
    "                                bias=bias, \n",
    "                                device=device, \n",
    "                                dtype=dtype,\n",
    "                                quant_config=quant_config['out_proj'], calibration=calibration)\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def forward(self, query, attn_mask=None):\n",
    "        q, k, v = self.in_proj(query).chunk(3, dim=-1)\n",
    "        \n",
    "        tgt_len, bsz, embed_dim = query.shape\n",
    "        head_dim = embed_dim // self.num_heads\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        k = k.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        v = v.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        o = torch.zeros_like(q)\n",
    "        all_row_sums = torch.zeros((bsz * self.num_heads, tgt_len, 1), dtype = self.dtype, device = self.device)\n",
    "        all_row_maxes = torch.full((bsz * self.num_heads, tgt_len, 1), self.max_neg_value, dtype = self.dtype, device = self.device)\n",
    "\n",
    "        num_tiles = math.ceil(tgt_len / self.token_tile_size)\n",
    "        if attn_mask is not None and attn_mask.ndim == 2:\n",
    "            #attn_mask = rearrange(attn_mask, 'b n -> 1 1 b n')\n",
    "            mask = attn_mask.bool()\n",
    "            #print(\"attn_mask shape:\", attn_mask.shape)\n",
    "\n",
    "        if attn_mask is None:\n",
    "            col_masks = (None,) * num_tiles\n",
    "            mask = (col_masks,) * num_tiles \n",
    "        else:\n",
    "            mask = ((mask,) * num_tiles) if mask.shape[-2] == 1 else mask.split(self.token_tile_size, dim = -2)\n",
    "            #print(\"attn_mask shape1:\", attn_mask.shape)\n",
    "            mask = tuple(((row_mask,) * num_tiles) if row_mask.shape[-1] == 1 else row_mask.split(self.token_tile_size, dim = -1) for row_mask in mask)\n",
    "\n",
    "        B, Nt, E = q.shape\n",
    "        scale = self.scale_qtzr(self.scale)\n",
    "        row_splits = zip(\n",
    "            q.split(self.token_tile_size, dim = -2),\n",
    "            o.split(self.token_tile_size, dim = -2),\n",
    "            mask,\n",
    "            all_row_sums.split(self.token_tile_size, dim = -2),\n",
    "            all_row_maxes.split(self.token_tile_size, dim = -2),\n",
    "        )\n",
    "        #attn_weight_debug = torch.zeros((self.num_heads, tgt_len, tgt_len), dtype = self.dtype, device = self.device)\n",
    "        #exp_weight_debug = torch.zeros((self.num_heads, tgt_len, tgt_len), dtype = self.dtype, device = self.device)\n",
    "        #row_max_debug = torch.zeros((self.num_heads, tgt_len, tgt_len), dtype = self.dtype, device = self.device)\n",
    "        #row_sum_debug = torch.zeros((self.num_heads, tgt_len, tgt_len), dtype = self.dtype, device = self.device)\n",
    "        #with open(\"K.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, k.reshape(-1, tgt_len*head_dim).detach().numpy(), fmt='%.6f')\n",
    "        #with open(\"Q.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, q.reshape(-1, tgt_len*head_dim).detach().numpy(), fmt='%.6f')\n",
    "        #with open(\"V.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, v.reshape(-1, tgt_len*head_dim).detach().numpy(), fmt='%.6f')\n",
    "        for i, (qc, oc, row_mask, row_sums, row_maxes) in enumerate(row_splits):\n",
    "            col_splits = zip(\n",
    "                k.split(self.token_tile_size, dim = -2),\n",
    "                v.split(self.token_tile_size, dim = -2),\n",
    "                row_mask\n",
    "            )\n",
    "            for j, (kc, vc, col_mask) in enumerate(col_splits):\n",
    "                attn_weights = torch.einsum('... i d, ... j d -> ... i j', qc, kc) * scale\n",
    "                if col_mask is not None:\n",
    "                #    #print(\"col_mask:\", ~col_mask)\n",
    "                    attn_weights.masked_fill_(col_mask, -1000000)\n",
    "                block_row_maxes = attn_weights.amax(dim = -1, keepdims = True)\n",
    "                new_row_maxes = torch.maximum(row_maxes, block_row_maxes)\n",
    "                #row_max_debug[:, i*self.token_tile_size:(i+1)*self.token_tile_size, j*self.token_tile_size:(j+1)*self.token_tile_size] = new_row_maxes\n",
    "                att_weights = attn_weights - new_row_maxes\n",
    "                #attn_weight_debug[:, i*self.token_tile_size:(i+1)*self.token_tile_size, j*self.token_tile_size:(j+1)*self.token_tile_size] = attn_weights\n",
    "                att_weights = self.exp_input_qtzr(att_weights)\n",
    "                exp_weights = torch.exp(att_weights)\n",
    "                exp_weights = self.exp_output_qtzr(exp_weights)\n",
    "                if col_mask is not None:\n",
    "                    exp_weights.masked_fill_(col_mask, 0.0)\n",
    "                #exp_weight_debug[:, i*self.token_tile_size:(i+1)*self.token_tile_size, j*self.token_tile_size:(j+1)*self.token_tile_size] = exp_weights\n",
    "                block_row_sums = exp_weights.sum(dim = -1, keepdims = True).clamp(min = 1e-10)\n",
    "                exp_values = torch.einsum('... i j, ... j d -> ... i d', exp_weights, vc)\n",
    "                exp_row_max_diff = self.exp_input_qtzr(row_maxes - new_row_maxes)\n",
    "                exp_row_max_diff = self.exp_output_qtzr(torch.exp(exp_row_max_diff))\n",
    "                new_row_sums = self.row_sum_qtzr(exp_row_max_diff * row_sums + block_row_sums)\n",
    "                #row_sum_debug[:, i*self.token_tile_size:(i+1)*self.token_tile_size, j*self.token_tile_size:(j+1)*self.token_tile_size] = new_row_sums\n",
    "                oc.mul_(exp_row_max_diff)\n",
    "                oc.add_(exp_values)\n",
    "                self.attn_out_qtzr.forward_inplace(oc)\n",
    "                \n",
    "                row_maxes.copy_(new_row_maxes)\n",
    "                row_sums.copy_(new_row_sums)\n",
    "            new_row_sums = self.inv_input_qtzr(new_row_sums)\n",
    "            oc.mul_(self.inv_output_qtzr(torch.reciprocal(new_row_sums + 1e-10)))\n",
    "            self.attn_out_qtzr.forward_inplace(oc)\n",
    "        #with open(\"O.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, o.reshape(-1, tgt_len*head_dim).detach().numpy(), fmt='%.6f')\n",
    "        attn_output = o.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        #with open(\"attn_weights.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, attn_weight_debug.reshape(-1, tgt_len*tgt_len).detach().numpy(), fmt='%.6f')\n",
    "        #with open(\"exp_weights.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, exp_weight_debug.reshape(-1, tgt_len*tgt_len).detach().numpy(), fmt='%.6f')\n",
    "        #with open(\"row_max.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, row_max_debug.reshape(-1, tgt_len*tgt_len).detach().numpy(), fmt='%.6f')\n",
    "        #with open(\"row_sum.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, row_sum_debug.reshape(-1, tgt_len*tgt_len).detach().numpy(), fmt='%.6f')\n",
    "        return attn_output\n",
    "        \n",
    "\n",
    "    \n",
    "class QLayerNorm(torch.nn.LayerNorm):\n",
    "    def __init__(self, \n",
    "                 normalized_shape:Tuple[int, ...],\n",
    "                 quant_config:dict=None,\n",
    "                 calibration=False):\n",
    "        super(QLayerNorm, self).__init__(normalized_shape)\n",
    "        self.input_qtzr = TorchQuantizer(**quant_config['input'], calibration=calibration)\n",
    "        self.scale_qtzr = TorchQuantizer(**quant_config['scale'], calibration=calibration)\n",
    "        self.bias_qtzr = TorchQuantizer(**quant_config['bias'], calibration=calibration)\n",
    "        self.output_qtzr = TorchQuantizer(**quant_config['output'], calibration=calibration)\n",
    "        self.mean_qtzr = TorchQuantizer(**quant_config['input'], calibration=calibration)\n",
    "        self.var_input_qtzr = TorchQuantizer(**quant_config['var_input'], rounding='TRUNCATE', saturation='SAT', calibration=calibration)\n",
    "        self.var_output_qtzr = TorchQuantizer(**quant_config['var_output'], saturation='SAT', calibration=calibration)\n",
    "        self.inv_embed_dim = torch.tensor(1.0 / self.normalized_shape[-1])\n",
    "        self.dim_qtzr = TorchQuantizer(bitwidth=18, int_bitwidth=0, signed=False, calibration=calibration)\n",
    "        self.inv_embed_dim = self.dim_qtzr(self.inv_embed_dim)\n",
    "    def forward(self, x):\n",
    "        x = self.input_qtzr(x)\n",
    "        xmean = x.sum(dim=-1, keepdim=True)\n",
    "        xmean.mul_(self.inv_embed_dim)\n",
    "        xmean = self.mean_qtzr(xmean)\n",
    "        #import numpy as np\n",
    "        #with open(\"xmean.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, xmean[:,0,:].detach().numpy(), fmt='%.6f')\n",
    "        xsqr = x**2\n",
    "        xsqrsum = xsqr.sum(dim=-1, keepdim=True)\n",
    "        xsqrsum.mul_(self.inv_embed_dim)\n",
    "        #with open(\"xvar.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, xsqrsum[:,0,:].detach().numpy(), fmt='%.6f')\n",
    "        xvar = xsqrsum - xmean**2\n",
    "        xvar = self.var_input_qtzr(xvar)\n",
    "        xvar = 1.0 / torch.sqrt(xvar+1e-8)\n",
    "        xvar = self.var_output_qtzr(xvar)\n",
    "        #with open(\"xvarout.txt\", 'a') as f:\n",
    "        #    np.savetxt(f, xvar[:,0,:].detach().numpy(), fmt='%.6f')\n",
    "        xnorm = (x - xmean) * xvar\n",
    "        weight = self.scale_qtzr(self.weight)\n",
    "        xnorm.mul_(weight)\n",
    "        bias = self.bias_qtzr(self.bias)\n",
    "        xnorm.add_(bias)\n",
    "        xnorm = self.output_qtzr(xnorm)\n",
    "        return xnorm\n",
    "    \n",
    "class QFeedForward(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim: int, \n",
    "                 hidden_dim: int,\n",
    "                 bias: bool = True, \n",
    "                 device: str = 'cpu', \n",
    "                 dtype: torch.dtype = torch.float64,\n",
    "                 quant_config: dict = None,\n",
    "                 calibration: bool = False):\n",
    "        super(QFeedForward, self).__init__()\n",
    "        self.in_proj = QLinear(embed_dim, \n",
    "                               hidden_dim, \n",
    "                               bias=bias, \n",
    "                               device=device, \n",
    "                               dtype=dtype,\n",
    "                               quant_config=quant_config['in_proj'], calibration=calibration)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.out_proj = QLinear(hidden_dim, \n",
    "                                embed_dim, \n",
    "                                bias=bias, \n",
    "                                device=device, \n",
    "                                dtype=dtype,\n",
    "                                quant_config=quant_config['out_proj'], calibration=calibration)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.in_proj(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "class QTransformerEncoderLayer(nn.TransformerEncoderLayer):\n",
    "    def __init__(self, \n",
    "                 embed_dim: int, \n",
    "                 num_heads: int, \n",
    "                 hidden_dim: int, \n",
    "                 dropout: float = 0.0, \n",
    "                 activation: str = 'relu', \n",
    "                 norm_first: bool = True, \n",
    "                 device: str = 'cpu', \n",
    "                 dtype: torch.dtype = torch.float64,\n",
    "                 quant_config: dict = None,\n",
    "                 calibration: bool = False,\n",
    "                 src_mask: torch.Tensor = None):\n",
    "        super(QTransformerEncoderLayer, self).__init__(embed_dim, \n",
    "                                                       num_heads, \n",
    "                                                       hidden_dim, \n",
    "                                                       dropout, \n",
    "                                                       activation, \n",
    "                                                       norm_first)\n",
    "        self.self_attn = QFlashMultiheadAttention(embed_dim,\n",
    "                                                    num_heads,\n",
    "                                                    device=device,\n",
    "                                                    dtype=dtype,\n",
    "                                                    quant_config=quant_config['self_attn'], calibration=calibration)\n",
    "        self.feedforward = QFeedForward(embed_dim,\n",
    "                                    hidden_dim,\n",
    "                                    device=device,\n",
    "                                    dtype=dtype,\n",
    "                                    quant_config=quant_config['ffn'], calibration=calibration)\n",
    "        self.norm1 = QLayerNorm(embed_dim,\n",
    "                                quant_config=quant_config['norm1'], calibration=calibration)\n",
    "        self.norm2 = QLayerNorm(embed_dim,\n",
    "                                quant_config=quant_config['norm2'], calibration=calibration)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm_first = norm_first\n",
    "        self.input_qtzr = TorchQuantizer(**quant_config['input'], calibration=calibration)\n",
    "        self.src_mask = src_mask\n",
    "\n",
    "    def forward(self, \n",
    "                src: torch.Tensor, \n",
    "                src_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        src = self.input_qtzr(src)\n",
    "        if self.norm_first:\n",
    "            #print(\"src:\", src)\n",
    "            #with open(\"src_norm1_in.txt\", 'a') as f:\n",
    "            #    np.savetxt(f, src[:,0,:].detach().numpy(), fmt='%.6f')\n",
    "            src_norm = self.norm1(src)\n",
    "            #import numpy as np\n",
    "            #with open(\"src_norm1.txt\", 'a') as f:\n",
    "            #    np.savetxt(f, src_norm[:,0,:].detach().numpy(), fmt='%.6f')\n",
    "            src2 = self.self_attn(src_norm, attn_mask=src_mask)\n",
    "            #with open(\"src_mha.txt\", 'a') as f:\n",
    "            #    np.savetxt(f, src2[:,0,:].detach().numpy(), fmt='%.6f')\n",
    "            #print(\"src_attn:\", src2)\n",
    "            src = src + self.dropout(src2)\n",
    "            #print(\"src:\", src)\n",
    "            src_norm = self.norm2(src)\n",
    "            #print(\"src_norm2:\", src_norm)\n",
    "            #with open(\"src_norm2.txt\", 'a') as f:\n",
    "            #    np.savetxt(f, src_norm[:,0,:].detach().numpy(), fmt='%.6f')\n",
    "            src2 = self.feedforward(src_norm)\n",
    "            #print(\"src2:\", src2)\n",
    "            #with open(\"src_ffn.txt\", 'a') as f:\n",
    "            #    np.savetxt(f, src2[:,0,:].detach().numpy(), fmt='%.6f')\n",
    "            src = src + self.dropout(src2)\n",
    "        else:\n",
    "            src2 = self.self_attn(src, attn_mask=src_mask)\n",
    "            src = src + self.dropout(src2)\n",
    "            src = self.norm1(src)\n",
    "            src2 = self.feedforward(src)\n",
    "            src = src + self.dropout(src2)\n",
    "            src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class QTransformerEncoder(nn.TransformerEncoder):\n",
    "    def __init__(self,\n",
    "                 encoder_layer: List[QTransformerEncoderLayer],\n",
    "                 num_layers: int,\n",
    "                 norm: QLayerNorm,\n",
    "                 dtype: torch.dtype = torch.float64):\n",
    "        super(QTransformerEncoder, self).__init__(encoder_layer[0], num_layers, norm)\n",
    "        self.layer_list = encoder_layer\n",
    "        self.norm = norm\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, \n",
    "                src: torch.Tensor, \n",
    "                mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        output = src\n",
    "        for mod in self.layer_list:\n",
    "            output = mod(output, src_mask=mask)\n",
    "        output = self.norm(output)\n",
    "        return output\n",
    "    \n",
    "    def transfer_weights(self, \n",
    "                         model: nn.Module):\n",
    "        for i, layer in enumerate(self.layer_list):\n",
    "            layer.norm1.weight.data = model.transformer_encoder.layers[i].norm1.weight.type(self.dtype)\n",
    "            layer.norm1.bias.data = model.transformer_encoder.layers[i].norm1.bias.type(self.dtype)\n",
    "            layer.norm2.weight.data = model.transformer_encoder.layers[i].norm2.weight.type(self.dtype)\n",
    "            layer.norm2.bias.data = model.transformer_encoder.layers[i].norm2.bias.type(self.dtype)\n",
    "            layer.self_attn.in_proj.weight.data = model.transformer_encoder.layers[i].self_attn.in_proj_weight.type(self.dtype)\n",
    "            layer.self_attn.in_proj.bias.data = model.transformer_encoder.layers[i].self_attn.in_proj_bias.type(self.dtype)\n",
    "            layer.self_attn.out_proj.weight.data = model.transformer_encoder.layers[i].self_attn.out_proj.weight.type(self.dtype)\n",
    "            layer.self_attn.out_proj.bias.data = model.transformer_encoder.layers[i].self_attn.out_proj.bias.type(self.dtype)\n",
    "            layer.feedforward.in_proj.weight.data = model.transformer_encoder.layers[i].linear1.weight.type(self.dtype)\n",
    "            layer.feedforward.in_proj.bias.data = model.transformer_encoder.layers[i].linear1.bias.type(self.dtype)\n",
    "            layer.feedforward.out_proj.weight.data = model.transformer_encoder.layers[i].linear2.weight.type(self.dtype)\n",
    "            layer.feedforward.out_proj.bias.data = model.transformer_encoder.layers[i].linear2.bias.type(self.dtype)\n",
    "        self.norm.weight.data = model.transformer_encoder.norm.weight.type(self.dtype)\n",
    "        self.norm.bias.data = model.transformer_encoder.norm.bias.type(self.dtype)\n",
    "\n",
    "def calibrate_transformer(qmodel: QTransformerEncoder, \n",
    "                          quant_config: dict, \n",
    "                          calibration_data: torch.Tensor,\n",
    "                          calibration_mask: torch.Tensor\n",
    "                          ) -> dict:\n",
    "    with torch.no_grad():\n",
    "        qmodel.eval()\n",
    "        qy = qmodel(calibration_data, mask=calibration_mask)\n",
    "        for i, layer in enumerate(qmodel.layer_list):\n",
    "            #print(\"Calibrating layer:\", id(layer.norm1.input_qtzr.max_int_bits))\n",
    "            #print(\"Calibrating:\", layer.norm1.input_qtzr.max_int_bits)\n",
    "            quant_config[i]['norm1']['input']['int_bitwidth'] = layer.norm1.input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm1']['scale']['int_bitwidth'] = layer.norm1.scale_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm1']['bias']['int_bitwidth'] = layer.norm1.bias_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm1']['output']['int_bitwidth'] = layer.norm1.output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm1']['var_input']['int_bitwidth'] = layer.norm1.var_input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm1']['var_output']['int_bitwidth'] = layer.norm1.var_output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm2']['input']['int_bitwidth'] = layer.norm2.input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm2']['scale']['int_bitwidth'] = layer.norm2.scale_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm2']['bias']['int_bitwidth'] = layer.norm2.bias_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm2']['output']['int_bitwidth'] = layer.norm2.output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm2']['var_input']['int_bitwidth'] = layer.norm2.var_input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['norm2']['var_output']['int_bitwidth'] = layer.norm2.var_output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['in_proj']['input']['int_bitwidth'] = layer.self_attn.in_proj.input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['in_proj']['weight']['int_bitwidth'] = layer.self_attn.in_proj.weight_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['in_proj']['bias']['int_bitwidth'] = layer.self_attn.in_proj.bias_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['in_proj']['output']['int_bitwidth'] = layer.self_attn.in_proj.output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['out_proj']['input']['int_bitwidth'] = layer.self_attn.out_proj.input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['out_proj']['weight']['int_bitwidth'] = layer.self_attn.out_proj.weight_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['out_proj']['bias']['int_bitwidth'] = layer.self_attn.out_proj.bias_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['out_proj']['output']['int_bitwidth'] = layer.self_attn.out_proj.output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['row_sum']['int_bitwidth'] = layer.self_attn.row_sum_qtzr.max_int_bits.item()\n",
    "            #quant_config[i]['self_attn']['exp_input']['int_bitwidth'] = layer.self_attn.exp_input_qtzr.max_int_bits.item()\n",
    "            #quant_config[i]['self_attn']['exp_output']['int_bitwidth'] = layer.self_attn.exp_output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['inv_input']['int_bitwidth'] = layer.self_attn.inv_input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['self_attn']['inv_output']['int_bitwidth'] = layer.self_attn.inv_output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['ffn']['in_proj']['input']['int_bitwidth'] = layer.feedforward.in_proj.input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['ffn']['in_proj']['weight']['int_bitwidth'] = layer.feedforward.in_proj.weight_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['ffn']['in_proj']['bias']['int_bitwidth'] = layer.feedforward.in_proj.bias_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['ffn']['in_proj']['output']['int_bitwidth'] = layer.feedforward.in_proj.output_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['ffn']['out_proj']['input']['int_bitwidth'] = layer.feedforward.out_proj.input_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['ffn']['out_proj']['weight']['int_bitwidth'] = layer.feedforward.out_proj.weight_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['ffn']['out_proj']['bias']['int_bitwidth'] = layer.feedforward.out_proj.bias_qtzr.max_int_bits.item()\n",
    "            quant_config[i]['ffn']['out_proj']['output']['int_bitwidth'] = layer.feedforward.out_proj.output_qtzr.max_int_bits.item()\n",
    "        quant_config['norm']['input']['int_bitwidth'] = qmodel.norm.input_qtzr.max_int_bits.item()\n",
    "        quant_config['norm']['scale']['int_bitwidth'] = qmodel.norm.scale_qtzr.max_int_bits.item()\n",
    "        quant_config['norm']['bias']['int_bitwidth'] = qmodel.norm.bias_qtzr.max_int_bits.item()\n",
    "        quant_config['norm']['output']['int_bitwidth'] = qmodel.norm.output_qtzr.max_int_bits.item()\n",
    "        quant_config['norm']['var_input']['int_bitwidth'] = qmodel.norm.var_input_qtzr.max_int_bits.item()\n",
    "        quant_config['norm']['var_output']['int_bitwidth'] = qmodel.norm.var_output_qtzr.max_int_bits.item()\n",
    "    return quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "#load quantization config\n",
    "def load_transformer_quant_config(quant_config_path: str = \"./quant_config.json\",\n",
    "                                  norm_quant_config_path: str = \"./norm_quant_config.json\",\n",
    "                                  num_layers: int = 1) -> dict:\n",
    "    with open(quant_config_path, 'r') as f:\n",
    "        quant_config = json.load(f)\n",
    "    with open(\"./norm_quant_config.json\", 'r') as f:\n",
    "        norm_quant_config = json.load(f)\n",
    "    transformer_quant_config = {}\n",
    "    for i in range(num_layers):\n",
    "        transformer_quant_config[i] = copy.deepcopy(quant_config)\n",
    "    transformer_quant_config['norm'] = copy.deepcopy(norm_quant_config)\n",
    "    return transformer_quant_config\n",
    "\n",
    "\n",
    "transformer_quant_config = load_transformer_quant_config(num_layers=4)\n",
    "pprint(transformer_quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def gen_init_state(num_layers:int) -> Dict[str, int]:\n",
    "    state = {}\n",
    "    for i in range(num_layers):\n",
    "        state.update({'layers_'+str(i)+'_norm1.Precision.var_table': 18,\n",
    "                      'layers_'+str(i)+'_norm1.VarTableSize': 10,\n",
    "                      'layers_'+str(i)+'_norm1.Precision.result': 18,\n",
    "                      'layers_'+str(i)+'_self_attn.Precision.exp_table': 18,\n",
    "                      'layers_'+str(i)+'_self_attn.ExpTableSize': 10,\n",
    "                      'layers_'+str(i)+'_self_attn.Precision.inv_table': 18,\n",
    "                      'layers_'+str(i)+'_self_attn.InvTableSize': 10,\n",
    "                      'layers_'+str(i)+'_self_attn.Precision.in_proj_out': 18,\n",
    "                      'layers_'+str(i)+'_self_attn.Precision.out_proj_in': 18,\n",
    "                      'layers_'+str(i)+'_self_attn.Precision.in_proj_weight': 18,\n",
    "                      'layers_'+str(i)+'_self_attn.Precision.out_proj_weight': 18,\n",
    "                      'layers_'+str(i)+'_self_attn.Precision.result': 18,\n",
    "                      'layers_'+str(i)+'_add1.Precision.result': 18,\n",
    "                      'layers_'+str(i)+'_norm2.Precision.var_table': 18,\n",
    "                      'layers_'+str(i)+'_norm2.VarTableSize': 10,\n",
    "                      'layers_'+str(i)+'_norm2.Precision.result': 18,\n",
    "                      'layers_'+str(i)+'_ffn.Precision.in_proj_weight': 18,\n",
    "                      'layers_'+str(i)+'_ffn.Precision.out_proj_weight': 18,\n",
    "                      'layers_'+str(i)+'_ffn.Precision.hidden': 18,\n",
    "                      'layers_'+str(i)+'_ffn.Precision.result': 18,\n",
    "                      'layers_'+str(i)+'_add2.Precision.result': 18})\n",
    "    state.update({'norm.Precision.var_table': 18,\n",
    "                  'norm.VarTableSize': 10,\n",
    "                  'norm.Precision.result': 18})\n",
    "    return state\n",
    "\n",
    "import re\n",
    "def sync_hls_config(hls_config:dict, \n",
    "                    state:dict) -> dict:\n",
    "    for key in state.keys():\n",
    "        subkey = key.split('.')\n",
    "        if len(subkey) == 3:\n",
    "            match = re.match(r'(ap_ufixed|ap_fixed|fixed|ufixed)<(\\d+),(-?\\d+)(?:,(\\w+)(?:,(\\w+)(?:,(\\d+))?)?)?>', hls_config['LayerName'][subkey[0]][subkey[1]][subkey[2]])\n",
    "            base_type, total_bits, integer_bits, rounding, saturation, sat_bits = match.groups()\n",
    "            if 'table' in subkey[2]:\n",
    "                hls_config['LayerName'][subkey[0]][subkey[1]][subkey[2]] = f'{base_type}<{state[key]},{integer_bits},{rounding},{saturation},{sat_bits}>'\n",
    "            else:\n",
    "                hls_config['LayerName'][subkey[0]][subkey[1]][subkey[2]] = f'{base_type}<{state[key]},{integer_bits},{rounding}>'\n",
    "        elif len(subkey) == 2:\n",
    "            hls_config['LayerName'][subkey[0]][subkey[1]] = int(2**state[key])\n",
    "    return hls_config\n",
    "\n",
    "def sync_quant_config(quant_config:dict, \n",
    "                      state:dict) -> dict:\n",
    "    for key in state.keys():\n",
    "        subkey = key.split('.')\n",
    "        layername = subkey[0]\n",
    "        if layername != 'norm':\n",
    "            layeridx = int(layername.split('_')[1])\n",
    "            if layername.endswith('self_attn'):\n",
    "                layername = 'self_attn'\n",
    "            else:\n",
    "                layername = layername.split('_')[2]\n",
    "        else:\n",
    "            if varname == 'var_table_size':\n",
    "                quant_config[layername]['var_input']['bitwidth'] = state[key]\n",
    "            elif varname == 'var_table':\n",
    "                quant_config[layername]['var_output']['bitwidth'] = state[key]\n",
    "            elif varname == 'result':\n",
    "                quant_config[layername]['output']['bitwidth'] = state[key]\n",
    "            continue\n",
    "        if subkey[1] == 'Precision':\n",
    "            varname = subkey[2]\n",
    "        else:\n",
    "            varname = subkey[1]\n",
    "\n",
    "        if 'norm' in layername:\n",
    "            if varname == 'VarTableSize':\n",
    "                quant_config[layeridx][layername]['var_input']['bitwidth'] = state[key]\n",
    "            elif varname == 'var_table':\n",
    "                quant_config[layeridx][layername]['var_output']['bitwidth'] = state[key]\n",
    "            elif varname == 'result':\n",
    "                quant_config[layeridx][layername]['output']['bitwidth'] = state[key]\n",
    "                #if layername == 'norm1':\n",
    "                #    quant_config[layeridx]['self_attn']['in_proj']['input']['bitwidth'] = state[key]\n",
    "                #elif layername == 'norm2':\n",
    "                #    quant_config[layeridx]['ffn']['in_proj']['input']['bitwidth'] = state[key]\n",
    "        elif 'self_attn' in layername:\n",
    "            if varname == 'ExpTableSize':\n",
    "                quant_config[layeridx][layername]['exp_input']['bitwidth'] = state[key]\n",
    "            elif varname == 'exp_table':\n",
    "                quant_config[layeridx][layername]['exp_output']['bitwidth'] = state[key]\n",
    "            elif varname == 'InvTableSize':\n",
    "                quant_config[layeridx][layername]['inv_input']['bitwidth'] = state[key]\n",
    "            elif varname == 'inv_table':\n",
    "                quant_config[layeridx][layername]['inv_output']['bitwidth'] = state[key]\n",
    "            elif varname == 'in_proj_out':\n",
    "                quant_config[layeridx][layername]['in_proj']['output']['bitwidth'] = state[key]\n",
    "            elif varname == 'out_proj_in':\n",
    "                quant_config[layeridx][layername]['out_proj']['input']['bitwidth'] = state[key]\n",
    "            elif varname == 'in_proj_weight':\n",
    "                quant_config[layeridx][layername]['in_proj']['weight']['bitwidth'] = state[key]\n",
    "            elif varname == 'out_proj_weight':\n",
    "                quant_config[layeridx][layername]['out_proj']['weight']['bitwidth'] = state[key]\n",
    "            elif varname == 'result':\n",
    "                quant_config[layeridx][layername]['out_proj']['output']['bitwidth'] = state[key]\n",
    "                #quant_config[layeridx]['norm2']['input']['bitwidth'] = state[key]\n",
    "        elif 'ffn' in layername:\n",
    "            if varname == 'in_proj_weight':\n",
    "                quant_config[layeridx][layername]['in_proj']['weight']['bitwidth'] = state[key]\n",
    "            elif varname == 'out_proj_weight':\n",
    "                quant_config[layeridx][layername]['out_proj']['weight']['bitwidth'] = state[key]\n",
    "            elif varname == 'hidden':\n",
    "                quant_config[layeridx][layername]['in_proj']['output']['bitwidth'] = state[key]\n",
    "                quant_config[layeridx][layername]['out_proj']['input']['bitwidth'] = state[key]\n",
    "            elif varname == 'result':\n",
    "                quant_config[layeridx][layername]['out_proj']['output']['bitwidth'] = state[key]\n",
    "                #if quant_config.get(layeridx+1) is not None:\n",
    "                #    quant_config[layeridx+1]['norm1']['input']['bitwidth'] = state[key]\n",
    "                #if quant_config.get('norm') is not None:\n",
    "                #    quant_config['norm']['input']['bitwidth'] = state[key]\n",
    "        elif 'add1' in layername:\n",
    "            if varname == 'result':\n",
    "                quant_config[layeridx]['norm2']['input']['bitwidth'] = state[key]\n",
    "        elif 'add2' in layername:\n",
    "            if varname == 'result':\n",
    "                if quant_config.get(layeridx+1) is not None:\n",
    "                    quant_config[layeridx+1]['norm2']['input']['bitwidth'] = state[key]\n",
    "                elif quant_config.get('norm') is not None:\n",
    "                    quant_config['norm']['input']['bitwidth'] = state[key]\n",
    "    return quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask_labels = np.load('./model/mask_labels.npy')\n",
    "src = np.load('./model/src.npy')\n",
    "mask_labels = torch.tensor(mask_labels)\n",
    "src = torch.tensor(src, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[0], calibration=True),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[1], calibration=True),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[2], calibration=True),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[3], calibration=True)], \n",
    "                             4, \n",
    "                             QLayerNorm(182, quant_config=transformer_quant_config['norm'], calibration=True),\n",
    "                             dtype=torch.float64)\n",
    "qmodel.transfer_weights(model)\n",
    "#calibration\n",
    "with torch.no_grad():\n",
    "    qmodel.eval()\n",
    "    transformer_quant_config = calibrate_transformer(qmodel, transformer_quant_config, src[0:1].permute(1,0,2), model.src_mask)\n",
    "\n",
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[0], calibration=False),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[1], calibration=False),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[2], calibration=False),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[3], calibration=False)], \n",
    "                             4, \n",
    "                             QLayerNorm(182, quant_config=transformer_quant_config['norm'], calibration=False),\n",
    "                             dtype=torch.float64)\n",
    "qmodel.transfer_weights(model)\n",
    "with torch.no_grad():\n",
    "    qmodel.eval()\n",
    "    qy = qmodel(src[0:1].permute(1,0,2), mask=mask_labels)\n",
    "    print(qy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "state = gen_init_state(4)\n",
    "\n",
    "state['layers_0_norm2.Precision.result'] = 18\n",
    "config = sync_hls_config(config, state)\n",
    "transformer_quant_config = sync_quant_config(transformer_quant_config, state)\n",
    "pprint(transformer_quant_config)\n",
    "pprint(config)\n",
    "!rm -rf ./hls/ndt\n",
    "hls_model = hls4ml.converters.convert_from_pytorch_model(\n",
    "                                                            model,\n",
    "                                                            [[1, 180, 182]],\n",
    "                                                            output_dir='./hls/ndt',\n",
    "                                                            project_name='myproject',\n",
    "                                                            backend='Vitis',\n",
    "                                                            #part='xcu250-figd2104-2L-e',\n",
    "                                                            part='xcu55c-fsvh2892-2L-e',\n",
    "                                                            #board='alveo-u55c',\n",
    "                                                            hls_config=config,\n",
    "                                                            io_type='io_tile_stream',\n",
    "                                                        )\n",
    "hls_model.compile()\n",
    "qmodel = QTransformerEncoder([QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[0], calibration=False),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[1], calibration=False),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[2], calibration=False),\n",
    "                              QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[3], calibration=False)], \n",
    "                             4, \n",
    "                             QLayerNorm(182, quant_config=transformer_quant_config['norm'], calibration=False),\n",
    "                             dtype=torch.float64)\n",
    "qmodel.transfer_weights(model)\n",
    "import os \n",
    "if os.path.exists('./src_norm2.txt'):\n",
    "    os.remove('./src_norm2.txt')\n",
    "if os.path.exists('./src_ffn.txt'):\n",
    "    os.remove('./src_ffn.txt')\n",
    "if os.path.exists('./src_norm1_in.txt'):\n",
    "    os.remove('./src_norm1_in.txt')\n",
    "with torch.no_grad():\n",
    "    qmodel.eval()\n",
    "    qy = qmodel(src[0:1].permute(1,0,2), mask=model.src_mask)\n",
    "    print(qy)\n",
    "    hls_y = hls_model.predict(src[0:1].numpy())\n",
    "    print(hls_y)\n",
    "    #check if the output is closer enough\n",
    "    assert np.allclose(qy.flatten().detach().numpy(), hls_y, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False\n",
    "import random\n",
    "state = gen_init_state(4)\n",
    "for key in state.keys():\n",
    "    print('---------------------------------')\n",
    "    print(\"key\", key)\n",
    "    print(\"bits\", state[key])\n",
    "    state[key] += random.randint(-3, 3)\n",
    "    print(\"bits\", state[key])\n",
    "#state['layers_0_ffn.Precision.result'] = 11\n",
    "    config = sync_hls_config(config, state)\n",
    "    transformer_quant_config = sync_quant_config(transformer_quant_config, state)\n",
    "    hls_model = hls4ml.converters.convert_from_pytorch_model(\n",
    "                                                                model,\n",
    "                                                                [[1, 180, 182]],\n",
    "                                                                output_dir='./hls/ndt',\n",
    "                                                                project_name='myproject',\n",
    "                                                                backend='Vitis',\n",
    "                                                                #part='xcu250-figd2104-2L-e',\n",
    "                                                                part='xcu55c-fsvh2892-2L-e',\n",
    "                                                                #board='alveo-u55c',\n",
    "                                                                hls_config=config,\n",
    "                                                                io_type='io_tile_stream',\n",
    "                                                            )\n",
    "    hls_model.compile()\n",
    "\n",
    "    qmodel = QTransformerEncoder([QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[0], calibration=False),\n",
    "                                  QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[1], calibration=False),\n",
    "                                  QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[2], calibration=False),\n",
    "                                  QTransformerEncoderLayer(182, 2, 128, quant_config=transformer_quant_config[3], calibration=False)], \n",
    "                                 4, \n",
    "                                 QLayerNorm(182, quant_config=transformer_quant_config['norm'], calibration=False),\n",
    "                                 dtype=torch.float64)\n",
    "    qmodel.transfer_weights(model)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qmodel.eval()\n",
    "        qy = qmodel(src[0:1].permute(1,0,2), mask=model.src_mask)\n",
    "        print(qy)\n",
    "        hls_y = hls_model.predict(src[0:1].numpy())\n",
    "        print(hls_y)\n",
    "        #check if the output is closer enough\n",
    "        assert np.allclose(qy.flatten().detach().numpy(), hls_y, atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-hls4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
